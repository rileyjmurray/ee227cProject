%\documentclass[12pt]{article}
%\usepackage{fullpage}
%\usepackage[]{mathtools}
%\usepackage{amsthm}
%\DeclareMathOperator{\Tr}{Tr}
%\usepackage[]{thmtools}
%\usepackage[dvipsnames]{xcolor}
%\declaretheorem{theorem}
%\declaretheoremstyle[
%	bodyfont=\normalfont, 
%	spaceabove=0.5cm]{defFormat}
%\declaretheoremstyle[
%	postheadspace=1cm,
%	bodyfont=\normalfont, 
%	spaceabove=0.5cm]{inLineDefFormat}
%\declaretheoremstyle[
%	spaceabove=0.5cm, 
%	spacebelow=0.5cm,
%	postheadspace=1cm]{namedTheorem}
%\declaretheorem[
%	numberwithin=section, 
%	style=defFormat, 
%	shaded]{definition}
%\declaretheorem[
%	style=inLineDefFormat, 
%	sibling=definition,
%	shaded,
%	name=Definition]{ILdefinition}
%\declaretheorem[
%	numbered=no, 
%	style=namedTheorem, 
%	shaded, 
%	name=The Unique Games Conjecture]{ugc}
%\declaretheorem[numbered=no, style=remark]{remark}
%\usepackage[]{bbm}
%\usepackage[]{amssymb}
%\begin{document}

%\parindent 0pt
%\parskip 8pt

\section{Introduction to Constraint Satisfaction Problems}\label{sec:introToCSP}
There are conflicting conventions regarding what is and is not a CSP. This section is devoted to defining ``CSP"'s in the way that is usually meant by the theoretical computer science community. A great help in getting {\it ourselves} oriented in the broad topic of CSP's and CSP approximations were lecture notes from Ryan O' Donnell's course at CMU \cite{Ryan}.
\subsection{Getting Oriented with CSP's}

There is a great deal of research going into the approximabilty of constraint satisfaction problems, and some claims regarding CSP's can seem quite sensational. 
The list of points below gives some facts relating to CSP's to help orient the reader.

\begin{itemize}
\item At their core, CSP's are \textit{optimization} problems. But as we will see, CSP's can be used to approach \textit{decision} problems. The reader is advised that some literature on CSP's conflate these two problem types.
\item The objective of a CSP is to satisfy as many constraints as possible; there is only one constraint that is ``safe" from violation: all variables must be in some simple, discrete domain.
\item Most CSP's are NP-Hard to solve optimally. Because of this, discussion of CSP's centers on developing approximate solutions to these problems.
\item Semidefinite Programming is the primary technique for CSP approximation.
\item The ability to generate a near optimal-solution for a CSP is intimately related to an open problem in computer science known as the ``Unique Games Conjecture."
\end{itemize}

\subsection{The CSP Framework and CSP Instances}
Define the \textit{arity} of an indicator function $R$ as the number of arguments it takes, and denote the arity by $\text{ar}(R)$.

\begin{definition}
\textbf{The CSP Framework} \\
Let $D$ be a finite domain of fixed cardinality $q$. 
Let $R$ denote an indicator function over $D$ with arity $r \leq k$ (i.e. $R:D^r \to \{0,1\}$). 
Let $\Gamma$ be a possibly exhaustive set of such functions. 
$D$ and $\Gamma$ define a \textit{class} of problems which we denote CSP($\Gamma$).
\label{def:CSPframework}
\end{definition}

We usually write $D = \{0,1,\ldots,q-1\}$, although the elements of $D$ can serve as \textit{labels}, without any of the algebraic structure implied by the use of integers.

\textbf{Examples}
\begin{itemize}
\item Max-Cut : For a graph $G = (V,E)$, label each vertex either ``$0$" or ``$1$" to maximize the number of edges connecting vertices with different labels.   
Here $D = \{0,1\}$, and all constraints are of the form $\{v_i \neq v_j\}$. We could write $\Gamma = \{\neq\}$ as the ``not equal" operator. 
\item Graph Coloring : Find an as-consistent-as-possible coloring for a graph using at most $q$ colors. 
Our domain is $D = \{0,1,\ldots,q-1\}$, and $\Gamma$ is again $\{\neq\}$. 
Note that Max-Cut is equivalent to graph coloring with $q = 2$.
\item Max $k-$SAT : $D = \{0,1\}$, $\Gamma$ is all disjunctions of \textit{up to} $k$ literals.
\item $3-$CSP : $D = \{0,1\}$, $\Gamma$ is all relations on up to three binary variables.
\end{itemize}

\begin{definition}
\textbf{CSP Instance}\\
An instance $\mathcal{C}$ of CSP($\Gamma$) is characterized by a set of $n$ variables (denoted $V$), a set of $m$ constraints (denoted $C$), and $m$ positive weights (one for each constraint). We write this as a triple $\mathcal{C} = (V,C,W)$.
Every constraint $C_i \in C$ has the form $C_i = (R_i,S_i)$ where $R_i \in \Gamma$ and $S_i$ (said to be the \textit{scope} of $C_i$) is a possibly ordered list of \text{ar}($R_i$) variables. 

If $F$ (a mapping $ V\to D$) is a given assignment of variables, then $R_i(F(S_i))$ equals either 1 or 0, in which case constraint $C_i$ is said to be ``satisfied" or ``not satisfied" respectively.
The objective is to maximize the weighted sum of satisfied constraints $\sum_{i = 1}^m w_i R_i(F(S_i))$. For a given $F$ we call this value as  $\text{Val}_{\mathcal{C}}[F]$.

$\mathcal{C}$ is said to be \textit{satisfiable} if $\forall i, ~ R_i(F(S_i)) = 1$ at optimality.
\label{def:CSPinstance}
\end{definition}

\subsubsection{Discussion of Definition \ref{def:CSPinstance}}
\textbf{Remarks on the objective function - }Without loss of generality, we may take the weights to sum to 1. 
When we do this, we can interpret the weights as probabilities and introduce a random variable $\tilde{C}$  which takes values in the set of constraints, i.e., it takes values in $C$. In particular, $\tilde{C} = C_i$ with probabiilty $w_i$. We denote this as $ \tilde{C} \sim W$, where $W$ stands for the probability mass function for the random variable $\tilde{C}$. This random variable view comes in handy later on. Next we define the random variable $R_F(\tilde{C}) = R_i(F(S_i))$ with probability $w_i$. And thus in this notation, we can write our objective as to maximize the expectation as expressed below -
\begin{equation} \label{expfirst}
\max_{F}  \text{Val}_{\mathcal{C}}[F] = \max_F \mathbb{E}_{\tilde{C} \sim W}\left[R_F(\tilde{C})\right] = \sum_{i=1}^m w_i R_i(F(S_i))
\end{equation}


\textbf{Remarks on the constraints - }In almost all optimization paradigms, ``constraints" are by their very definition \textit{inviolable}. 
Definition \ref{def:CSPinstance} departs from this convention in that a feasible solution is not required to satisfy any of a CSP's ``constraints." 
Are CSP's then accurately understood as unconstrained optimization? 
Not quite. 
The CSP Framework has one and only one inviolable constraint (and that constraint is precisely what makes them difficult): each of the $n$ variables $x_i, ~ i \in\{1,\ldots,n\}$ belongs to the discrete domain $D$. 

\subsection{Complexity for Solving and Approximating CSP's}
One of the most surprising results in CSP's and approximation algorithms is the lack of a consistent relationship between the difficulty of solving a problem optimally, and the difficulty of approximating the problem.

Take 3-SAT for example. 
It is well known that it is NP-Hard to find a satisfiable assignment to a 3-SAT instance even \textit{given} that it is satisfiable. 
It is equivalent to say that it is NP-Hard to $(1,1)$ approximate ``Max 3-SAT." 
In spite of this, Max 3-SAT has a $(7/8,1)$ polynomial time approximation algorithm \cite{Zwick1997}, and a $(0.784\beta,\beta)$ approximation algorithm \cite{AsanoAndWilliamson2002}.
From this we might expect that approximating an optimization problem (Max 3-SAT) is easier than solving the corresponding decision problem (3-SAT). 
Alas, this is not always the case. 

For example, the 2-SAT decision problem can be solved in polynomial time, but Max 2-SAT is NP-Hard to approximate by better than 0.955. \cite{Hastad2001}
The Max-Cut problem is yet another example of this phenomenon. 
There is a polynomial time algorithm for determining whether or not a Max-Cut instance is satisfyable\footnote{one need only check whether the graph is bipartite}, 
but the best known approximation algorithm for Max-Cut in the general case is a $(0.878\beta,\beta)$ approximation \cite{gwFirstMaxCutSDP}. 
It has been proven that it is NP-Hard to approximate Max-Cut by a factor better than 0.942 \cite{Trevisan2000}, 
but it may yet be NP-Hard to do better than 0.878! 

Whether existing approximation algorithms can be improved for a huge swath of combinatorial problems (including Max-Cut), depends heavily on the truth of the Unique Games Conjecture. 
We address this next.

\subsection{Hardness of Approximation: The Unique Games Conjecture} 
In 2002, Subhash Khot published a paper entitled \textit{On the Power of Unique 2-Prover 1-Round Games} \cite{Khot}. 
In his paper, Khot put forward the Unique Games Conjecture- currently one of the most important open problems in theoretical computer science.

It can be difficult to get a handle on what the Unique Games Conjecture claims. 
The purpose of this section is to provide the reader with the background necessary to understand how the Unique Games Conjecture relates to CSP approximation. 
This will become critically important in subsequent sections on SDP relaxations of CSP's.

\begin{itemize}
\item The UGC is a statement of the hardness of approximation: 
for some problems, it is NP-Hard to determine whether every solution for a given instance is \textit{extremely poor}, or whether an \textit{almost perfect} solution exists for the instance.
\item If true, the UGC would imply that many already existing approximation algorithms cannot be improved upon. 
This includes approximation algorithms which are not for CSP's and do not use semidefinite programming.
\end{itemize}

\subsubsection{What is a ``Unique Game"?}

A ``unique game" is a game in the game-theory sense that relates to Probabilistically Checkable Proofs (PCP's). 
It is a slightly less general version of ``2-Prover 1-Round games." 
Below we list the three conditions of 2-Prover 1-Round games, and then the additional condition that makes such a game ``unique."
\begin{itemize}
\item The game pits two \textit{provers} against a \textit{verifier}.
\item The verifier asks one question of each prover.\footnote{Questions are drawn from appropriate probability distributions.}
\item Given the answers returned by the two provers, the verifier returns ``True" or ``False."
\item[\textbf{*}] The answer of one prover \textit{uniquely} determines the answer of the other prover.
\end{itemize}

\subsubsection{What does UGC have to do with CSP's or combinatorial optimization?}

Khot posed the ``game" in the UGC in three equivalent ways
\begin{itemize}
\item The 2-Prover 1-Round game with uniqueness (specified above).
\item A CSP where the constraints to be satisfied are a system of linear equations in two variables, modulo 2.
\item A new graph-theoretic CSP called ``Label Cover."\footnote{Khot's game chose a particular type of Label Cover problem where the graph is bipartite, but subsequent discussions of Unique Games do not emphasize this characteristic. }
\end{itemize}

If it is claimed that an algorithm is for ``Unique Games," the reader would do well to clarify which formulation the authors work with. Because graph theory has a larger research community than those working with Probabilistically Checkable Proofs, several researchers (including Khot himself) primarily use the Label Cover formulation of Unique Games in discourse. 

The interested reader is strongly recommended to read Khot's original paper for a precise definitions for Proababilistically Checkable Proofs, the Two-Prover One-Round ``unique" game, and the Label Cover problem. But suffice it to say

\begin{ugc}
For arbitrarily small constants $\epsilon$, $\delta > 0$, there exists a constant $k = k(\epsilon,\delta)$ such that it is NP-Hard to determine whether a unique Label Cover instance with label sets of size $k$ at satisfies \textit{at least} $1-\epsilon$ or \textit{at most} $\delta$ constraints.
\end{ugc}




%\end{document}
