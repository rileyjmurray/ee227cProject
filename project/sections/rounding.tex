\section{[Relatively] Simple SDP Rounding Schemes}\label{sec:rounding}
This section presents the following
\begin{itemize}
\item A framework for understanding the output of Basic SDP that is useful in developing randomized rounding schemes. 
\item The Goemans Williamson Algorithm for Max-Cut and Max 2-SAT.
\item A more general rounding scheme for Basic SDP that applies to any \textit{Unique} CSP.
\end{itemize}

\subsection{A Framework for Rounding Basic SDP} 
Note that the solution of the relaxed problem gives us a PSD matrix and probability distributions over constraints. 
The next challenge is to make a ``consistent draw" from this distribution and analyzing the expected value of CSP resulting from such draws. 

We concretely summarize various angles, that we discussed briefly before, to look at canonical SDP relaxations for a CSP :
\begin{itemize}
\item Pseudo-indicator random variables which satisfy the first and second moment consistency constraints. 
This perspective is arguably the best for understanding the SDP. 
\item Pseudo-indicator random variables which satisfy the second moment constraints. 
This perspective is arguably the best when constructing SDP solutions by hand.
\item Vectors $\{ y_{(v, \ell)} : (v, \ell) \in V \times D \}$ satisfying the first and second moment consistency constraints.  
This perspective is the one that's actually used computationally, on a computer. 
\item Jointly Gaussian Pseudo-indicator random variables which satisfy the consistent first and second moment constraints. 
This perspective is suited for developing``SDP Rounding algorithms". 
\end{itemize}
We now move to the quintessential SDP and randomized-rounding based approximation algorithm for Max-Cut.

\subsection{Goemans Williamson Algorithm}\label{GWAlgo}
By now, we know how to relax Max-Cut to an SDP, and assuming efficient poly-time algorithm for SDP solver, we have a solution for the SDP formulation. Now, we want to somehow convert that back to \textit{good solution} for Max-Cut. 

We will borrow notation and variables from Section \ref{maxcut}. We remind the reader that the notation for the example is different than our Basic SDP and also $Y \in \R^{d \times |V|}, (d \leq |V|)$ and $\Si \in \R^{|V| \times |V|}$ as against our Basic SDP where we have $|V| \cdot |D|$ columns in place of $|V|$ for either matrix.

Next we discuss the rounding discussed in \cite{gwFirstMaxCutSDP}. We assume that given the optimal solution of the SDPformulation of  Max-Cut  in  Example \ref{maxcut}  (which is not Basic!) we have already found $Y$ such that $Y\trans Y = \Si$ (which can be done in polynomial time using Cholesky Decomposition).

Our aim is to partition the various vertices in two groups. We can do this by partitioning the vectors $y_i$. We do this by cutting the vectors $\{y_i\}_{i \in V}$ with a random hyper plane through origin such that all vectors on one side of the hyperplane correspond to variables in one partition, and the rest are classified into other partition, thereby giving us a cut. 

\begin{algorithm}\textbf{Goemans Williamson Rounding}
\begin{enumerate}
\item Draw a vector $\hat{n} \in \R^n$ (where $\hat{n}$ denotes the normal to the hyperplane) from any rotationally symmetric distribution. 
\item Set $x_i = \sgn{\hat{n} \cdot y_i} \in \{-1, 1 \}$.
\end{enumerate}
\end{algorithm}

{\bf Analysis of the Rounding} \\
For a given graph $G = (V, E)$, with $V =\{1, \ldots, n\}$, and $E$ the set of edges, fix $(i, j) \in E$. Then the probability that the edge $(i, j)$ is cut by the hyperplane is same as the probability that hyperplane splits $y_i$, and $y_j$. 
Now consider just simply the $2D$ plane containing $y_i, y_j$. Since the hyperplane was chosen from a rotationally symmetric distribution, the probability that it cuts these two vectors is same as that a random diameter in the circle containing $y_i, y_j$, lies in between the angle $\theta$ of these two vectors. Thus, 
\al{
\Pb[(i, j) \mbox{ gets cut} ] &= \frac{\theta}{\pi} =\frac{{\rm cos\inv}\lrb{y_i \cdot y_j}}{\pi} = \frac{{\rm cos\inv}\lrb{\Si_{ij}}}{\pi}\\
\Ex[\mbox{Weight of edges cut}] &= \sum_{[i, j] \in E}w_{ij} \Pb[(i, j) \mbox{ gets cut} ] = \sum_{[i, j] \in E}w_{ij} \frac{{\rm cos\inv}\lrb{\Si_{ij}}}{\pi}
}
Now recall that 
\al{
\mbox{SDPOpt} = \sum_{[i, j] \in E} w_{ij} \lrb{\frac{1}{2}-\frac{1}{2}\Si_{ij}} \geq \mbox{Opt}.
}
So, if we find $\alpha$ such that 
\al{
\frac{{\rm cos\inv}\lrb{\Si_{ij}}}{\pi} = \alpha\ \lrb{\frac{1}{2}-\frac{1}{2}\Si_{ij}} \fa \Si_{ij} \in [-1, 1]
}
then we can conclude 
\al{
\Ex[\mbox{cut val}]\geq \alpha \mbox{SDPOpt} \geq \alpha \mbox{Opt}
}
Solving it numerically we get that $\alpha = 0.87856$ works.
\begin{remark}
$\Ex[\mbox{Cut}]  \geq 0.87856 \mbox{\ SDPOpt} \geq 0.87856 \mbox{\ Opt }$
\end{remark} 
 
\subsection{Gaussian Rounding for Unique CSP's}

We begin the graph-theoretic definition of the \textit{uniqueness property} of a CSP.
\begin{definition}{\bf Unique Game}\\
A unique game is any game that can be stated in the following terms: given a constraint graph $G = (V, E)$, an assignment function $F: V \rightarrow D$ and a set of permutations $\pi_{vv'}$ on $D = \{0, \ldots, q-1 \}$ (for all edges $(v, v')$). Each permutations $\pi_{vv'}$ defines the constraint $\pi_{vv'}(F(v'))=F(v)$. The goal is to find an assignment $F$ so as to maximize the number of satisfied constraints.
\end{definition}

Note that Max-Cut is a Unique CSP since the $\{\neq\}$ constraint on a domain of size 2 satisfies the uniqueness property. Graph coloring (a generalization of Max-Cut to a larger domain) has the same constraint type ($\{\neq\}$) but is \textit{non-unique} since for any value $\ell_1$ the first argument takes on, there are $q-1$ values for $\ell_2$ that satisfy the $\{\neq\}$ constraint. Graph coloring then, is a non-unique 2-CSP. As a broader class of problems, any CSP with arity greater than 2 is necessarily \textit{non-unique}. This implies that $k-$SAT with $k > 2$ is a non-unique CSP.

Next we discuss the rounding scheme from \cite{cmm06}, which is a generalization of the Goemans Williamson Algorithm for the Max-Cut to a Rounding Algorithm for Unique Games. 

Given a Unique Game $G = (V,E)$ with edge weights $W = \{w_{vv'} | (v, v') \in E \}$, we formulate it as a CSP with constraints $\C$ (using appropriate choice of constraint functions $R$) and weights $W$. Then, we solve the SDP relaxation for this CSP instance and decompose the solution $\Sigma = U\trans U$ with $U \in \R^{N \times N}$ with $N = |V| \cdot |D|$, using Cholesky decomposition (which is polytime). 

Denote by $[x]_r$ the function that rounds $x$ up or down depending on whether the fractional part of $x$ is greater or less than $r$. 
Note that if $r$ is uniformly distributed in the interval $[0,1]$, then the expected value of $[x]_r$ is $x$. 

\begin{algorithm}\textbf{Rounding \textit{Unique} CSP's}
\begin{enumerate}
\item Pick a number $r$ in the interval $[0, 1]$ uniformly at random.
\item Pick random independent Gaussian vectors $g_1, \ldots, g_{2q}$ with independent components distributed as $\mathcal{N}(0, 1)$.
\item For each vertex $v$:
\begin{enumerate}
\item Find normalized vectors $\tilde{u}_{\ell} = {u_{\ell}}/{||u_\ell||_2^2}$
\item Set $s_{u_\ell} = [2q \cdot ||u_\ell||_2^2]_r$, for $\ell=0, \ldots, q-1$.
\item For each $\ell$, project $s_{u_\ell}$ vectors $g_1, \ldots, g_{s_{u_\ell}}$ to $\tilde{u}_\ell$:
\al{
\xi_{u_\ell, s} = g_s\trans \tilde{u}_\ell, 1 \leq s \leq s_{u_\ell}.
}
Hence for each variable $u$, there are $s_{u_0}+\ldots+s_{u_{q-1}}$ many $\xi$'s, call this set $\Xi_u$.
\item For each $u$, find the maximum magnitude member in $\Xi_u$ and let it be $\xi_{u_{\ell^*}, s^*}$. Assign
\al{
F(u) = \ell^*
}
\end{enumerate}
\end{enumerate}
\end{algorithm}
\begin{theorem}
If the optimal solution of the unique game satisfies $1-\epsilon$ fraction of constraints, then the above algorithm in expectation satisfied $1-O({\sqrt{\epsilon \log q }})$ fraction of constraints.
\end{theorem}
Proof of the above theorem is quite involved and we refer the reader to \cite{cmm06} for the analysis, which mainly involved tools and techniques from probability theory.
